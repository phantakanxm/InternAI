{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c93bf68",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, LayerNormalization, \n",
    "    MultiHeadAttention, Conv2D, Rescaling\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from PIL import ImageFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f63757",
   "metadata": {},
   "source": [
    "# Submission of models to meet the imagenet supported standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceffc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "NUM_LAYERS = 8\n",
    "EMBED_DIM = 384\n",
    "MLP_DIM = 4 * EMBED_DIM\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.1\n",
    "NUM_CLASSES = 1000\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "IMG_CHANNELS = 3\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f60ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "กำลังโหลดโมเดลจาก: /media/capybara/Data/dataset_vit/codeVIT/best_vit_model_1000_classes.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749187385.399184   25183 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7209 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:04:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โหลดโมเดลสำเร็จ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vision_transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vision_transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ patch_embed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)    │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)  │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)  │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)  │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)  │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)  │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)  │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)  │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ head_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">385,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ patch_embed (\u001b[38;5;33mConv2D\u001b[0m)            │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │       \u001b[38;5;34m295,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block (\u001b[38;5;33mEncoderBlock\u001b[0m)    │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_1 (\u001b[38;5;33mEncoderBlock\u001b[0m)  │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_2 (\u001b[38;5;33mEncoderBlock\u001b[0m)  │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_3 (\u001b[38;5;33mEncoderBlock\u001b[0m)  │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_4 (\u001b[38;5;33mEncoderBlock\u001b[0m)  │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_5 (\u001b[38;5;33mEncoderBlock\u001b[0m)  │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_6 (\u001b[38;5;33mEncoderBlock\u001b[0m)  │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_block_7 (\u001b[38;5;33mEncoderBlock\u001b[0m)  │ ?                      │     \u001b[38;5;34m1,774,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ head_norm (\u001b[38;5;33mLayerNormalization\u001b[0m)  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)               │           \u001b[38;5;34m768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification_head (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1000\u001b[0m)              │       \u001b[38;5;34m385,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,858,425</span> (171.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,858,425\u001b[0m (171.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,952,808</span> (57.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,952,808\u001b[0m (57.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,905,617</span> (114.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m29,905,617\u001b[0m (114.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังเตรียม TensorFlow dataset จาก: /media/capybara/Data/dataset_vit/testdata\n",
      "พบรูปภาพทั้งหมด 100000 รูป\n",
      "กำลังสร้างไฟล์ submission...\n",
      "กำลังประมวลผลรูปภาพ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3125 [00:00<?, ?it/s]I0000 00:00:1749187393.942408   25183 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "100%|██████████| 3125/3125 [14:18<00:00,  3.67it/s]2025-06-06 12:37:32.195514: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "100%|██████████| 3125/3125 [14:18<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังบันทึกไฟล์ submission ไปที่: submission_custom_model.txt\n",
      "สร้างไฟล์ 'submission_custom_model.txt' สำเร็จ\n",
      "เสร็จสิ้น\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class EncoderBlock(Model):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim, self.num_heads, self.mlp_dim, self.dropout_rate = embed_dim, num_heads, mlp_dim, dropout_rate\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6, name=\"norm1\")\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout_rate, name=\"multi_head_attention\")\n",
    "        self.dropout_mha_output = Dropout(dropout_rate)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6, name=\"norm2\")\n",
    "        self.mlp = Sequential([\n",
    "            Dense(mlp_dim, activation='gelu', name=\"mlp_dense_1\"),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(embed_dim, name=\"mlp_dense_2\"),\n",
    "            Dropout(dropout_rate)\n",
    "        ], name=\"mlp_block\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_norm1 = self.norm1(inputs)\n",
    "        attn_output = self.mha(query=x_norm1, value=x_norm1, key=x_norm1, training=training)\n",
    "        attn_output_dropped = self.dropout_mha_output(attn_output, training=training)\n",
    "        x_res1 = inputs + attn_output_dropped\n",
    "        x_norm2 = self.norm2(x_res1)\n",
    "        mlp_output = self.mlp(x_norm2, training=training)\n",
    "        x_res2 = x_res1 + mlp_output\n",
    "        return x_res2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EncoderBlock, self).get_config()\n",
    "        config.update({'embed_dim': self.embed_dim, 'num_heads': self.num_heads, 'mlp_dim': self.mlp_dim, 'dropout_rate': self.dropout_rate})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class VisionTransformer(Model):\n",
    "    def __init__(self, image_size=IMAGE_SIZE, num_classes=NUM_CLASSES, patch_size=PATCH_SIZE, embed_dim=EMBED_DIM,\n",
    "                 num_heads=NUM_HEADS, num_layers=NUM_LAYERS, mlp_dim=MLP_DIM, dropout_rate=DROPOUT_RATE, **kwargs):\n",
    "        super(VisionTransformer, self).__init__(**kwargs)\n",
    "        if isinstance(image_size, int): image_size = (image_size, image_size)\n",
    "        if isinstance(patch_size, int): patch_size = (patch_size, patch_size)\n",
    "        self.image_size, self.num_classes, self.patch_size = image_size, num_classes, patch_size\n",
    "        self.embed_dim, self.num_heads, self.num_layers = embed_dim, num_heads, num_layers\n",
    "        self.mlp_dim, self.dropout_rate = mlp_dim, dropout_rate\n",
    "        self.num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1])\n",
    "        self.cls_token = self.add_weight(name=\"cls_token\", shape=[1, 1, embed_dim], initializer=tf.keras.initializers.RandomNormal(stddev=0.02), trainable=True)\n",
    "        self.pos_embed = self.add_weight(name=\"position_embedding\", shape=[1, self.num_patches + 1, embed_dim], initializer=tf.keras.initializers.RandomNormal(stddev=0.02), trainable=True)\n",
    "        self.pos_dropout = Dropout(dropout_rate)\n",
    "        self.patch_embed = Conv2D(filters=embed_dim, kernel_size=patch_size, strides=patch_size, padding='valid', name=\"patch_embed\")\n",
    "        self.encoder_layers = [EncoderBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        self.norm_head = LayerNormalization(epsilon=1e-6, name=\"head_norm\")\n",
    "        self.head = Dense(num_classes, activation='softmax', name=\"classification_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        x = self.patch_embed(inputs)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.embed_dim))\n",
    "        cls_tokens = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
    "        x = tf.concat([cls_tokens, x], axis=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x, training=training)\n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x, training=training)\n",
    "        cls_token_output = self.norm_head(x[:, 0], training=training)\n",
    "        logits = self.head(cls_token_output, training=training)\n",
    "        return logits\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(VisionTransformer, self).get_config()\n",
    "        config.update({'image_size': self.image_size, 'num_classes': self.num_classes, 'patch_size': self.patch_size, 'embed_dim': self.embed_dim, 'num_heads': self.num_heads, 'num_layers': self.num_layers, 'mlp_dim': self.mlp_dim, 'dropout_rate': self.dropout_rate})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, decay_steps, warmup_steps, alpha=0.0, name=None):\n",
    "        super().__init__()\n",
    "        self.name, self.initial_learning_rate, self.decay_steps, self.warmup_steps, self.alpha = name, initial_learning_rate, decay_steps, warmup_steps, alpha\n",
    "        self.cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=self.initial_learning_rate, decay_steps=self.decay_steps - self.warmup_steps, alpha=self.alpha)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step_float = tf.cast(step, tf.float32)\n",
    "        warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "        def warmup_fn(): return (self.initial_learning_rate / warmup_steps_float) * step_float\n",
    "        def cosine_decay_fn(): return self.cosine_decay_schedule(step_float - warmup_steps_float)\n",
    "        return tf.cond(step_float < warmup_steps_float, warmup_fn, cosine_decay_fn)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"initial_learning_rate\": self.initial_learning_rate, \"decay_steps\": self.decay_steps, \"warmup_steps\": self.warmup_steps, \"alpha\": self.alpha, \"name\": self.name}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "try:\n",
    "    idx_to_ilsvrc_id_df = pd.read_csv(\"https://github.com/TheoBourdais/ImageNetSubmission/raw/main/src/idx_to_ILSVRC_ID.csv\")\n",
    "    idx_to_ilsvrc_id = {idx: ilsvrc_id for idx, ilsvrc_id in zip(idx_to_ilsvrc_id_df[\"idx\"], idx_to_ilsvrc_id_df[\"ILSVRC_ID\"])}\n",
    "except Exception as e:\n",
    "    print(f\"ไม่สามารถดาวน์โหลดไฟล์ Class Mappings ได้: {e}\"); exit()\n",
    "\n",
    "rescale_layer_only = Rescaling(1./255)\n",
    "\n",
    "def preprocess_image_tf(image_path, target_height=IMG_HEIGHT, target_width=IMG_WIDTH):\n",
    "    img_bytes = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
    "    img = tf.image.resize(img, [target_height, target_width])\n",
    "    return image_path, rescale_layer_only(img)\n",
    "\n",
    "def create_tf_dataset(image_folder_path, batch_size):\n",
    "    image_paths = sorted(tf.io.gfile.glob(os.path.join(image_folder_path, \"**\", \"*.JPEG\")))\n",
    "    if not image_paths:\n",
    "        for ext in [\"*.jpeg\", \"*.jpg\"]:\n",
    "            image_paths = sorted(tf.io.gfile.glob(os.path.join(image_folder_path, \"**\", ext)))\n",
    "            if image_paths: break\n",
    "    if not image_paths: raise ValueError(f\"ไม่พบไฟล์รูปภาพใน: {image_folder_path}\")\n",
    "    print(f\"พบรูปภาพทั้งหมด {len(image_paths)} รูป\")\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    image_dataset = filename_dataset.map(lambda x: preprocess_image_tf(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return image_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "def get_test_submission_tf(model, tf_dataloader):\n",
    "    all_filenames, all_predictions_np = [], []\n",
    "    print(\"กำลังประมวลผลรูปภาพ...\")\n",
    "    for batch_paths, batch_images in tqdm(tf_dataloader):\n",
    "        outputs = model(batch_images, training=False)\n",
    "        _, predicted_indices = tf.math.top_k(outputs, k=5)\n",
    "        all_filenames.extend([os.path.basename(p.numpy().decode('utf-8')) for p in batch_paths])\n",
    "        all_predictions_np.extend(predicted_indices.numpy())\n",
    "    mapped_predictions = [[idx_to_ilsvrc_id.get(i, \"n00000000\") for i in row] for row in all_predictions_np]\n",
    "    submission_df = pd.DataFrame(mapped_predictions, columns=[f\"choice {i}\" for i in range(1, 6)], index=all_filenames)\n",
    "    return submission_df.sort_index()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "    \n",
    "    keras_model_path = \"/media/capybara/Data/dataset_vit/codeVIT/best_vit_model_1000_classes.keras\"\n",
    "    print(f\"กำลังโหลดโมเดลจาก: {keras_model_path}\")\n",
    "\n",
    "    custom_objects = {\n",
    "        \"EncoderBlock\": EncoderBlock,\n",
    "        \"VisionTransformer\": VisionTransformer,\n",
    "        \"WarmupCosineDecay\": WarmupCosineDecay\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        model = load_model(keras_model_path, custom_objects=custom_objects)\n",
    "        print(\"โหลดโมเดลสำเร็จ\")\n",
    "    except Exception as e:\n",
    "        print(f\"เกิดข้อผิดพลาดในการโหลดโมเดล: {e}\"); exit()\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    validation_set_path = \"/media/capybara/Data/dataset_vit/testdata\"\n",
    "    \n",
    "    print(f\"กำลังเตรียม TensorFlow dataset จาก: {validation_set_path}\")\n",
    "    test_tf_dataset = create_tf_dataset(validation_set_path, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(\"กำลังสร้างไฟล์ submission...\")\n",
    "    submission_df = get_test_submission_tf(model, test_tf_dataset)\n",
    "\n",
    "    submission_file_path = \"submission_custom_model.txt\"\n",
    "    print(f\"กำลังบันทึกไฟล์ submission ไปที่: {submission_file_path}\")\n",
    "    submission_df.to_csv(submission_file_path, index=False, header=False, sep=\" \")\n",
    "    print(f\"สร้างไฟล์ '{submission_file_path}' สำเร็จ\")\n",
    "    print(\"เสร็จสิ้น\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
