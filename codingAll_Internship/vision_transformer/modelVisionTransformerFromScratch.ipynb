{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddb6669",
   "metadata": {},
   "source": [
    "# Check GPU if it is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a319ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(\"GPU Name: \", gpu_devices[0].name)\n",
    "    try:\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "        print(\"Matrix multiplication on GPU successful:\", c)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error during GPU test:\", e)\n",
    "else:\n",
    "    print(\"TensorFlow is NOT using a GPU. Training will be very slow.\")\n",
    "\n",
    "import numpy\n",
    "import scipy\n",
    "print(f\"NumPy version: {numpy.__version__}\")\n",
    "print(f\"SciPy version: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02387775",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0305f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization, MultiHeadAttention, Conv2D\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69d46c",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459de1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/media/capybara/Data/dataset_vit/archive'\n",
    "\n",
    "datapath = train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254543d",
   "metadata": {},
   "source": [
    "# Set the configuration for ViT structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2711bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ส่วนของการรันโมเดล ---\n",
    "IMAGE_SIZE = 224 # \n",
    "PATCH_SIZE = 16  # ขนาดของ patch\n",
    "BATCH_SIZE = 32 # อาจปรับ batch size ตามหน่วยความจำ\n",
    "NUM_CLASSES = 1000 # จำนวนคลาสใน dataset\n",
    "EMBED_DIM = 384 # ลดขนาดลงสำหรับการทดลอง\n",
    "NUM_HEADS = 8 # จำนวน attention heads\n",
    "NUM_LAYERS = 8 # จำนวน layer ของ encoder\n",
    "MLP_DIM = 4 * EMBED_DIM\n",
    "DROPOUT_RATE = 0.1\n",
    "EPOCHS = 50 # เพิ่มจำนวน epochs เพื่อให้โมเดลมีเวลาเรียนรู้"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed61069",
   "metadata": {},
   "source": [
    "# Preprocess Image before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1281167 files belonging to 1000 classes.\n",
      "Using 1229921 files for training.\n",
      "Found 1281167 files belonging to 1000 classes.\n",
      "Using 51246 files for validation.\n",
      "Class names: ['n01440764', 'n01443537', 'n01484850', 'n01491361', 'n01494475', 'n01496331', 'n01498041', 'n01514668', 'n01514859', 'n01518878', 'n01530575', 'n01531178', 'n01532829', 'n01534433', 'n01537544', 'n01558993', 'n01560419', 'n01580077', 'n01582220', 'n01592084', 'n01601694', 'n01608432', 'n01614925', 'n01616318', 'n01622779', 'n01629819', 'n01630670', 'n01631663', 'n01632458', 'n01632777', 'n01641577', 'n01644373', 'n01644900', 'n01664065', 'n01665541', 'n01667114', 'n01667778', 'n01669191', 'n01675722', 'n01677366', 'n01682714', 'n01685808', 'n01687978', 'n01688243', 'n01689811', 'n01692333', 'n01693334', 'n01694178', 'n01695060', 'n01697457', 'n01698640', 'n01704323', 'n01728572', 'n01728920', 'n01729322', 'n01729977', 'n01734418', 'n01735189', 'n01737021', 'n01739381', 'n01740131', 'n01742172', 'n01744401', 'n01748264', 'n01749939', 'n01751748', 'n01753488', 'n01755581', 'n01756291', 'n01768244', 'n01770081', 'n01770393', 'n01773157', 'n01773549', 'n01773797', 'n01774384', 'n01774750', 'n01775062', 'n01776313', 'n01784675', 'n01795545', 'n01796340', 'n01797886', 'n01798484', 'n01806143', 'n01806567', 'n01807496', 'n01817953', 'n01818515', 'n01819313', 'n01820546', 'n01824575', 'n01828970', 'n01829413', 'n01833805', 'n01843065', 'n01843383', 'n01847000', 'n01855032', 'n01855672', 'n01860187', 'n01871265', 'n01872401', 'n01873310', 'n01877812', 'n01882714', 'n01883070', 'n01910747', 'n01914609', 'n01917289', 'n01924916', 'n01930112', 'n01943899', 'n01944390', 'n01945685', 'n01950731', 'n01955084', 'n01968897', 'n01978287', 'n01978455', 'n01980166', 'n01981276', 'n01983481', 'n01984695', 'n01985128', 'n01986214', 'n01990800', 'n02002556', 'n02002724', 'n02006656', 'n02007558', 'n02009229', 'n02009912', 'n02011460', 'n02012849', 'n02013706', 'n02017213', 'n02018207', 'n02018795', 'n02025239', 'n02027492', 'n02028035', 'n02033041', 'n02037110', 'n02051845', 'n02056570', 'n02058221', 'n02066245', 'n02071294', 'n02074367', 'n02077923', 'n02085620', 'n02085782', 'n02085936', 'n02086079', 'n02086240', 'n02086646', 'n02086910', 'n02087046', 'n02087394', 'n02088094', 'n02088238', 'n02088364', 'n02088466', 'n02088632', 'n02089078', 'n02089867', 'n02089973', 'n02090379', 'n02090622', 'n02090721', 'n02091032', 'n02091134', 'n02091244', 'n02091467', 'n02091635', 'n02091831', 'n02092002', 'n02092339', 'n02093256', 'n02093428', 'n02093647', 'n02093754', 'n02093859', 'n02093991', 'n02094114', 'n02094258', 'n02094433', 'n02095314', 'n02095570', 'n02095889', 'n02096051', 'n02096177', 'n02096294', 'n02096437', 'n02096585', 'n02097047', 'n02097130', 'n02097209', 'n02097298', 'n02097474', 'n02097658', 'n02098105', 'n02098286', 'n02098413', 'n02099267', 'n02099429', 'n02099601', 'n02099712', 'n02099849', 'n02100236', 'n02100583', 'n02100735', 'n02100877', 'n02101006', 'n02101388', 'n02101556', 'n02102040', 'n02102177', 'n02102318', 'n02102480', 'n02102973', 'n02104029', 'n02104365', 'n02105056', 'n02105162', 'n02105251', 'n02105412', 'n02105505', 'n02105641', 'n02105855', 'n02106030', 'n02106166', 'n02106382', 'n02106550', 'n02106662', 'n02107142', 'n02107312', 'n02107574', 'n02107683', 'n02107908', 'n02108000', 'n02108089', 'n02108422', 'n02108551', 'n02108915', 'n02109047', 'n02109525', 'n02109961', 'n02110063', 'n02110185', 'n02110341', 'n02110627', 'n02110806', 'n02110958', 'n02111129', 'n02111277', 'n02111500', 'n02111889', 'n02112018', 'n02112137', 'n02112350', 'n02112706', 'n02113023', 'n02113186', 'n02113624', 'n02113712', 'n02113799', 'n02113978', 'n02114367', 'n02114548', 'n02114712', 'n02114855', 'n02115641', 'n02115913', 'n02116738', 'n02117135', 'n02119022', 'n02119789', 'n02120079', 'n02120505', 'n02123045', 'n02123159', 'n02123394', 'n02123597', 'n02124075', 'n02125311', 'n02127052', 'n02128385', 'n02128757', 'n02128925', 'n02129165', 'n02129604', 'n02130308', 'n02132136', 'n02133161', 'n02134084', 'n02134418', 'n02137549', 'n02138441', 'n02165105', 'n02165456', 'n02167151', 'n02168699', 'n02169497', 'n02172182', 'n02174001', 'n02177972', 'n02190166', 'n02206856', 'n02219486', 'n02226429', 'n02229544', 'n02231487', 'n02233338', 'n02236044', 'n02256656', 'n02259212', 'n02264363', 'n02268443', 'n02268853', 'n02276258', 'n02277742', 'n02279972', 'n02280649', 'n02281406', 'n02281787', 'n02317335', 'n02319095', 'n02321529', 'n02325366', 'n02326432', 'n02328150', 'n02342885', 'n02346627', 'n02356798', 'n02361337', 'n02363005', 'n02364673', 'n02389026', 'n02391049', 'n02395406', 'n02396427', 'n02397096', 'n02398521', 'n02403003', 'n02408429', 'n02410509', 'n02412080', 'n02415577', 'n02417914', 'n02422106', 'n02422699', 'n02423022', 'n02437312', 'n02437616', 'n02441942', 'n02442845', 'n02443114', 'n02443484', 'n02444819', 'n02445715', 'n02447366', 'n02454379', 'n02457408', 'n02480495', 'n02480855', 'n02481823', 'n02483362', 'n02483708', 'n02484975', 'n02486261', 'n02486410', 'n02487347', 'n02488291', 'n02488702', 'n02489166', 'n02490219', 'n02492035', 'n02492660', 'n02493509', 'n02493793', 'n02494079', 'n02497673', 'n02500267', 'n02504013', 'n02504458', 'n02509815', 'n02510455', 'n02514041', 'n02526121', 'n02536864', 'n02606052', 'n02607072', 'n02640242', 'n02641379', 'n02643566', 'n02655020', 'n02666196', 'n02667093', 'n02669723', 'n02672831', 'n02676566', 'n02687172', 'n02690373', 'n02692877', 'n02699494', 'n02701002', 'n02704792', 'n02708093', 'n02727426', 'n02730930', 'n02747177', 'n02749479', 'n02769748', 'n02776631', 'n02777292', 'n02782093', 'n02783161', 'n02786058', 'n02787622', 'n02788148', 'n02790996', 'n02791124', 'n02791270', 'n02793495', 'n02794156', 'n02795169', 'n02797295', 'n02799071', 'n02802426', 'n02804414', 'n02804610', 'n02807133', 'n02808304', 'n02808440', 'n02814533', 'n02814860', 'n02815834', 'n02817516', 'n02823428', 'n02823750', 'n02825657', 'n02834397', 'n02835271', 'n02837789', 'n02840245', 'n02841315', 'n02843684', 'n02859443', 'n02860847', 'n02865351', 'n02869837', 'n02870880', 'n02871525', 'n02877765', 'n02879718', 'n02883205', 'n02892201', 'n02892767', 'n02894605', 'n02895154', 'n02906734', 'n02909870', 'n02910353', 'n02916936', 'n02917067', 'n02927161', 'n02930766', 'n02939185', 'n02948072', 'n02950826', 'n02951358', 'n02951585', 'n02963159', 'n02965783', 'n02966193', 'n02966687', 'n02971356', 'n02974003', 'n02977058', 'n02978881', 'n02979186', 'n02980441', 'n02981792', 'n02988304', 'n02992211', 'n02992529', 'n02999410', 'n03000134', 'n03000247', 'n03000684', 'n03014705', 'n03016953', 'n03017168', 'n03018349', 'n03026506', 'n03028079', 'n03032252', 'n03041632', 'n03042490', 'n03045698', 'n03047690', 'n03062245', 'n03063599', 'n03063689', 'n03065424', 'n03075370', 'n03085013', 'n03089624', 'n03095699', 'n03100240', 'n03109150', 'n03110669', 'n03124043', 'n03124170', 'n03125729', 'n03126707', 'n03127747', 'n03127925', 'n03131574', 'n03133878', 'n03134739', 'n03141823', 'n03146219', 'n03160309', 'n03179701', 'n03180011', 'n03187595', 'n03188531', 'n03196217', 'n03197337', 'n03201208', 'n03207743', 'n03207941', 'n03208938', 'n03216828', 'n03218198', 'n03220513', 'n03223299', 'n03240683', 'n03249569', 'n03250847', 'n03255030', 'n03259280', 'n03271574', 'n03272010', 'n03272562', 'n03290653', 'n03291819', 'n03297495', 'n03314780', 'n03325584', 'n03337140', 'n03344393', 'n03345487', 'n03347037', 'n03355925', 'n03372029', 'n03376595', 'n03379051', 'n03384352', 'n03388043', 'n03388183', 'n03388549', 'n03393912', 'n03394916', 'n03400231', 'n03404251', 'n03417042', 'n03424325', 'n03425413', 'n03443371', 'n03444034', 'n03445777', 'n03445924', 'n03447447', 'n03447721', 'n03450230', 'n03452741', 'n03457902', 'n03459775', 'n03461385', 'n03467068', 'n03476684', 'n03476991', 'n03478589', 'n03481172', 'n03482405', 'n03483316', 'n03485407', 'n03485794', 'n03492542', 'n03494278', 'n03495258', 'n03496892', 'n03498962', 'n03527444', 'n03529860', 'n03530642', 'n03532672', 'n03534580', 'n03535780', 'n03538406', 'n03544143', 'n03584254', 'n03584829', 'n03590841', 'n03594734', 'n03594945', 'n03595614', 'n03598930', 'n03599486', 'n03602883', 'n03617480', 'n03623198', 'n03627232', 'n03630383', 'n03633091', 'n03637318', 'n03642806', 'n03649909', 'n03657121', 'n03658185', 'n03661043', 'n03662601', 'n03666591', 'n03670208', 'n03673027', 'n03676483', 'n03680355', 'n03690938', 'n03691459', 'n03692522', 'n03697007', 'n03706229', 'n03709823', 'n03710193', 'n03710637', 'n03710721', 'n03717622', 'n03720891', 'n03721384', 'n03724870', 'n03729826', 'n03733131', 'n03733281', 'n03733805', 'n03742115', 'n03743016', 'n03759954', 'n03761084', 'n03763968', 'n03764736', 'n03769881', 'n03770439', 'n03770679', 'n03773504', 'n03775071', 'n03775546', 'n03776460', 'n03777568', 'n03777754', 'n03781244', 'n03782006', 'n03785016', 'n03786901', 'n03787032', 'n03788195', 'n03788365', 'n03791053', 'n03792782', 'n03792972', 'n03793489', 'n03794056', 'n03796401', 'n03803284', 'n03804744', 'n03814639', 'n03814906', 'n03825788', 'n03832673', 'n03837869', 'n03838899', 'n03840681', 'n03841143', 'n03843555', 'n03854065', 'n03857828', 'n03866082', 'n03868242', 'n03868863', 'n03871628', 'n03873416', 'n03874293', 'n03874599', 'n03876231', 'n03877472', 'n03877845', 'n03884397', 'n03887697', 'n03888257', 'n03888605', 'n03891251', 'n03891332', 'n03895866', 'n03899768', 'n03902125', 'n03903868', 'n03908618', 'n03908714', 'n03916031', 'n03920288', 'n03924679', 'n03929660', 'n03929855', 'n03930313', 'n03930630', 'n03933933', 'n03935335', 'n03937543', 'n03938244', 'n03942813', 'n03944341', 'n03947888', 'n03950228', 'n03954731', 'n03956157', 'n03958227', 'n03961711', 'n03967562', 'n03970156', 'n03976467', 'n03976657', 'n03977966', 'n03980874', 'n03982430', 'n03983396', 'n03991062', 'n03992509', 'n03995372', 'n03998194', 'n04004767', 'n04005630', 'n04008634', 'n04009552', 'n04019541', 'n04023962', 'n04026417', 'n04033901', 'n04033995', 'n04037443', 'n04039381', 'n04040759', 'n04041544', 'n04044716', 'n04049303', 'n04065272', 'n04067472', 'n04069434', 'n04070727', 'n04074963', 'n04081281', 'n04086273', 'n04090263', 'n04099969', 'n04111531', 'n04116512', 'n04118538', 'n04118776', 'n04120489', 'n04125021', 'n04127249', 'n04131690', 'n04133789', 'n04136333', 'n04141076', 'n04141327', 'n04141975', 'n04146614', 'n04147183', 'n04149813', 'n04152593', 'n04153751', 'n04154565', 'n04162706', 'n04179913', 'n04192698', 'n04200800', 'n04201297', 'n04204238', 'n04204347', 'n04208210', 'n04209133', 'n04209239', 'n04228054', 'n04229816', 'n04235860', 'n04238763', 'n04239074', 'n04243546', 'n04251144', 'n04252077', 'n04252225', 'n04254120', 'n04254680', 'n04254777', 'n04258138', 'n04259630', 'n04263257', 'n04264628', 'n04265275', 'n04266014', 'n04270147', 'n04273569', 'n04275548', 'n04277352', 'n04285008', 'n04286575', 'n04296562', 'n04310018', 'n04311004', 'n04311174', 'n04317175', 'n04325704', 'n04326547', 'n04328186', 'n04330267', 'n04332243', 'n04335435', 'n04336792', 'n04344873', 'n04346328', 'n04347754', 'n04350905', 'n04355338', 'n04355933', 'n04356056', 'n04357314', 'n04366367', 'n04367480', 'n04370456', 'n04371430', 'n04371774', 'n04372370', 'n04376876', 'n04380533', 'n04389033', 'n04392985', 'n04398044', 'n04399382', 'n04404412', 'n04409515', 'n04417672', 'n04418357', 'n04423845', 'n04428191', 'n04429376', 'n04435653', 'n04442312', 'n04443257', 'n04447861', 'n04456115', 'n04458633', 'n04461696', 'n04462240', 'n04465501', 'n04467665', 'n04476259', 'n04479046', 'n04482393', 'n04483307', 'n04485082', 'n04486054', 'n04487081', 'n04487394', 'n04493381', 'n04501370', 'n04505470', 'n04507155', 'n04509417', 'n04515003', 'n04517823', 'n04522168', 'n04523525', 'n04525038', 'n04525305', 'n04532106', 'n04532670', 'n04536866', 'n04540053', 'n04542943', 'n04548280', 'n04548362', 'n04550184', 'n04552348', 'n04553703', 'n04554684', 'n04557648', 'n04560804', 'n04562935', 'n04579145', 'n04579432', 'n04584207', 'n04589890', 'n04590129', 'n04591157', 'n04591713', 'n04592741', 'n04596742', 'n04597913', 'n04599235', 'n04604644', 'n04606251', 'n04612504', 'n04613696', 'n06359193', 'n06596364', 'n06785654', 'n06794110', 'n06874185', 'n07248320', 'n07565083', 'n07579787', 'n07583066', 'n07584110', 'n07590611', 'n07613480', 'n07614500', 'n07615774', 'n07684084', 'n07693725', 'n07695742', 'n07697313', 'n07697537', 'n07711569', 'n07714571', 'n07714990', 'n07715103', 'n07716358', 'n07716906', 'n07717410', 'n07717556', 'n07718472', 'n07718747', 'n07720875', 'n07730033', 'n07734744', 'n07742313', 'n07745940', 'n07747607', 'n07749582', 'n07753113', 'n07753275', 'n07753592', 'n07754684', 'n07760859', 'n07768694', 'n07802026', 'n07831146', 'n07836838', 'n07860988', 'n07871810', 'n07873807', 'n07875152', 'n07880968', 'n07892512', 'n07920052', 'n07930864', 'n07932039', 'n09193705', 'n09229709', 'n09246464', 'n09256479', 'n09288635', 'n09332890', 'n09399592', 'n09421951', 'n09428293', 'n09468604', 'n09472597', 'n09835506', 'n10148035', 'n10565667', 'n11879895', 'n11939491', 'n12057211', 'n12144580', 'n12267677', 'n12620546', 'n12768682', 'n12985857', 'n12998815', 'n13037406', 'n13040303', 'n13044778', 'n13052670', 'n13054560', 'n13133613', 'n15075141']\n",
      "Class indices: {'n01440764': 0, 'n01443537': 1, 'n01484850': 2, 'n01491361': 3, 'n01494475': 4, 'n01496331': 5, 'n01498041': 6, 'n01514668': 7, 'n01514859': 8, 'n01518878': 9, 'n01530575': 10, 'n01531178': 11, 'n01532829': 12, 'n01534433': 13, 'n01537544': 14, 'n01558993': 15, 'n01560419': 16, 'n01580077': 17, 'n01582220': 18, 'n01592084': 19, 'n01601694': 20, 'n01608432': 21, 'n01614925': 22, 'n01616318': 23, 'n01622779': 24, 'n01629819': 25, 'n01630670': 26, 'n01631663': 27, 'n01632458': 28, 'n01632777': 29, 'n01641577': 30, 'n01644373': 31, 'n01644900': 32, 'n01664065': 33, 'n01665541': 34, 'n01667114': 35, 'n01667778': 36, 'n01669191': 37, 'n01675722': 38, 'n01677366': 39, 'n01682714': 40, 'n01685808': 41, 'n01687978': 42, 'n01688243': 43, 'n01689811': 44, 'n01692333': 45, 'n01693334': 46, 'n01694178': 47, 'n01695060': 48, 'n01697457': 49, 'n01698640': 50, 'n01704323': 51, 'n01728572': 52, 'n01728920': 53, 'n01729322': 54, 'n01729977': 55, 'n01734418': 56, 'n01735189': 57, 'n01737021': 58, 'n01739381': 59, 'n01740131': 60, 'n01742172': 61, 'n01744401': 62, 'n01748264': 63, 'n01749939': 64, 'n01751748': 65, 'n01753488': 66, 'n01755581': 67, 'n01756291': 68, 'n01768244': 69, 'n01770081': 70, 'n01770393': 71, 'n01773157': 72, 'n01773549': 73, 'n01773797': 74, 'n01774384': 75, 'n01774750': 76, 'n01775062': 77, 'n01776313': 78, 'n01784675': 79, 'n01795545': 80, 'n01796340': 81, 'n01797886': 82, 'n01798484': 83, 'n01806143': 84, 'n01806567': 85, 'n01807496': 86, 'n01817953': 87, 'n01818515': 88, 'n01819313': 89, 'n01820546': 90, 'n01824575': 91, 'n01828970': 92, 'n01829413': 93, 'n01833805': 94, 'n01843065': 95, 'n01843383': 96, 'n01847000': 97, 'n01855032': 98, 'n01855672': 99, 'n01860187': 100, 'n01871265': 101, 'n01872401': 102, 'n01873310': 103, 'n01877812': 104, 'n01882714': 105, 'n01883070': 106, 'n01910747': 107, 'n01914609': 108, 'n01917289': 109, 'n01924916': 110, 'n01930112': 111, 'n01943899': 112, 'n01944390': 113, 'n01945685': 114, 'n01950731': 115, 'n01955084': 116, 'n01968897': 117, 'n01978287': 118, 'n01978455': 119, 'n01980166': 120, 'n01981276': 121, 'n01983481': 122, 'n01984695': 123, 'n01985128': 124, 'n01986214': 125, 'n01990800': 126, 'n02002556': 127, 'n02002724': 128, 'n02006656': 129, 'n02007558': 130, 'n02009229': 131, 'n02009912': 132, 'n02011460': 133, 'n02012849': 134, 'n02013706': 135, 'n02017213': 136, 'n02018207': 137, 'n02018795': 138, 'n02025239': 139, 'n02027492': 140, 'n02028035': 141, 'n02033041': 142, 'n02037110': 143, 'n02051845': 144, 'n02056570': 145, 'n02058221': 146, 'n02066245': 147, 'n02071294': 148, 'n02074367': 149, 'n02077923': 150, 'n02085620': 151, 'n02085782': 152, 'n02085936': 153, 'n02086079': 154, 'n02086240': 155, 'n02086646': 156, 'n02086910': 157, 'n02087046': 158, 'n02087394': 159, 'n02088094': 160, 'n02088238': 161, 'n02088364': 162, 'n02088466': 163, 'n02088632': 164, 'n02089078': 165, 'n02089867': 166, 'n02089973': 167, 'n02090379': 168, 'n02090622': 169, 'n02090721': 170, 'n02091032': 171, 'n02091134': 172, 'n02091244': 173, 'n02091467': 174, 'n02091635': 175, 'n02091831': 176, 'n02092002': 177, 'n02092339': 178, 'n02093256': 179, 'n02093428': 180, 'n02093647': 181, 'n02093754': 182, 'n02093859': 183, 'n02093991': 184, 'n02094114': 185, 'n02094258': 186, 'n02094433': 187, 'n02095314': 188, 'n02095570': 189, 'n02095889': 190, 'n02096051': 191, 'n02096177': 192, 'n02096294': 193, 'n02096437': 194, 'n02096585': 195, 'n02097047': 196, 'n02097130': 197, 'n02097209': 198, 'n02097298': 199, 'n02097474': 200, 'n02097658': 201, 'n02098105': 202, 'n02098286': 203, 'n02098413': 204, 'n02099267': 205, 'n02099429': 206, 'n02099601': 207, 'n02099712': 208, 'n02099849': 209, 'n02100236': 210, 'n02100583': 211, 'n02100735': 212, 'n02100877': 213, 'n02101006': 214, 'n02101388': 215, 'n02101556': 216, 'n02102040': 217, 'n02102177': 218, 'n02102318': 219, 'n02102480': 220, 'n02102973': 221, 'n02104029': 222, 'n02104365': 223, 'n02105056': 224, 'n02105162': 225, 'n02105251': 226, 'n02105412': 227, 'n02105505': 228, 'n02105641': 229, 'n02105855': 230, 'n02106030': 231, 'n02106166': 232, 'n02106382': 233, 'n02106550': 234, 'n02106662': 235, 'n02107142': 236, 'n02107312': 237, 'n02107574': 238, 'n02107683': 239, 'n02107908': 240, 'n02108000': 241, 'n02108089': 242, 'n02108422': 243, 'n02108551': 244, 'n02108915': 245, 'n02109047': 246, 'n02109525': 247, 'n02109961': 248, 'n02110063': 249, 'n02110185': 250, 'n02110341': 251, 'n02110627': 252, 'n02110806': 253, 'n02110958': 254, 'n02111129': 255, 'n02111277': 256, 'n02111500': 257, 'n02111889': 258, 'n02112018': 259, 'n02112137': 260, 'n02112350': 261, 'n02112706': 262, 'n02113023': 263, 'n02113186': 264, 'n02113624': 265, 'n02113712': 266, 'n02113799': 267, 'n02113978': 268, 'n02114367': 269, 'n02114548': 270, 'n02114712': 271, 'n02114855': 272, 'n02115641': 273, 'n02115913': 274, 'n02116738': 275, 'n02117135': 276, 'n02119022': 277, 'n02119789': 278, 'n02120079': 279, 'n02120505': 280, 'n02123045': 281, 'n02123159': 282, 'n02123394': 283, 'n02123597': 284, 'n02124075': 285, 'n02125311': 286, 'n02127052': 287, 'n02128385': 288, 'n02128757': 289, 'n02128925': 290, 'n02129165': 291, 'n02129604': 292, 'n02130308': 293, 'n02132136': 294, 'n02133161': 295, 'n02134084': 296, 'n02134418': 297, 'n02137549': 298, 'n02138441': 299, 'n02165105': 300, 'n02165456': 301, 'n02167151': 302, 'n02168699': 303, 'n02169497': 304, 'n02172182': 305, 'n02174001': 306, 'n02177972': 307, 'n02190166': 308, 'n02206856': 309, 'n02219486': 310, 'n02226429': 311, 'n02229544': 312, 'n02231487': 313, 'n02233338': 314, 'n02236044': 315, 'n02256656': 316, 'n02259212': 317, 'n02264363': 318, 'n02268443': 319, 'n02268853': 320, 'n02276258': 321, 'n02277742': 322, 'n02279972': 323, 'n02280649': 324, 'n02281406': 325, 'n02281787': 326, 'n02317335': 327, 'n02319095': 328, 'n02321529': 329, 'n02325366': 330, 'n02326432': 331, 'n02328150': 332, 'n02342885': 333, 'n02346627': 334, 'n02356798': 335, 'n02361337': 336, 'n02363005': 337, 'n02364673': 338, 'n02389026': 339, 'n02391049': 340, 'n02395406': 341, 'n02396427': 342, 'n02397096': 343, 'n02398521': 344, 'n02403003': 345, 'n02408429': 346, 'n02410509': 347, 'n02412080': 348, 'n02415577': 349, 'n02417914': 350, 'n02422106': 351, 'n02422699': 352, 'n02423022': 353, 'n02437312': 354, 'n02437616': 355, 'n02441942': 356, 'n02442845': 357, 'n02443114': 358, 'n02443484': 359, 'n02444819': 360, 'n02445715': 361, 'n02447366': 362, 'n02454379': 363, 'n02457408': 364, 'n02480495': 365, 'n02480855': 366, 'n02481823': 367, 'n02483362': 368, 'n02483708': 369, 'n02484975': 370, 'n02486261': 371, 'n02486410': 372, 'n02487347': 373, 'n02488291': 374, 'n02488702': 375, 'n02489166': 376, 'n02490219': 377, 'n02492035': 378, 'n02492660': 379, 'n02493509': 380, 'n02493793': 381, 'n02494079': 382, 'n02497673': 383, 'n02500267': 384, 'n02504013': 385, 'n02504458': 386, 'n02509815': 387, 'n02510455': 388, 'n02514041': 389, 'n02526121': 390, 'n02536864': 391, 'n02606052': 392, 'n02607072': 393, 'n02640242': 394, 'n02641379': 395, 'n02643566': 396, 'n02655020': 397, 'n02666196': 398, 'n02667093': 399, 'n02669723': 400, 'n02672831': 401, 'n02676566': 402, 'n02687172': 403, 'n02690373': 404, 'n02692877': 405, 'n02699494': 406, 'n02701002': 407, 'n02704792': 408, 'n02708093': 409, 'n02727426': 410, 'n02730930': 411, 'n02747177': 412, 'n02749479': 413, 'n02769748': 414, 'n02776631': 415, 'n02777292': 416, 'n02782093': 417, 'n02783161': 418, 'n02786058': 419, 'n02787622': 420, 'n02788148': 421, 'n02790996': 422, 'n02791124': 423, 'n02791270': 424, 'n02793495': 425, 'n02794156': 426, 'n02795169': 427, 'n02797295': 428, 'n02799071': 429, 'n02802426': 430, 'n02804414': 431, 'n02804610': 432, 'n02807133': 433, 'n02808304': 434, 'n02808440': 435, 'n02814533': 436, 'n02814860': 437, 'n02815834': 438, 'n02817516': 439, 'n02823428': 440, 'n02823750': 441, 'n02825657': 442, 'n02834397': 443, 'n02835271': 444, 'n02837789': 445, 'n02840245': 446, 'n02841315': 447, 'n02843684': 448, 'n02859443': 449, 'n02860847': 450, 'n02865351': 451, 'n02869837': 452, 'n02870880': 453, 'n02871525': 454, 'n02877765': 455, 'n02879718': 456, 'n02883205': 457, 'n02892201': 458, 'n02892767': 459, 'n02894605': 460, 'n02895154': 461, 'n02906734': 462, 'n02909870': 463, 'n02910353': 464, 'n02916936': 465, 'n02917067': 466, 'n02927161': 467, 'n02930766': 468, 'n02939185': 469, 'n02948072': 470, 'n02950826': 471, 'n02951358': 472, 'n02951585': 473, 'n02963159': 474, 'n02965783': 475, 'n02966193': 476, 'n02966687': 477, 'n02971356': 478, 'n02974003': 479, 'n02977058': 480, 'n02978881': 481, 'n02979186': 482, 'n02980441': 483, 'n02981792': 484, 'n02988304': 485, 'n02992211': 486, 'n02992529': 487, 'n02999410': 488, 'n03000134': 489, 'n03000247': 490, 'n03000684': 491, 'n03014705': 492, 'n03016953': 493, 'n03017168': 494, 'n03018349': 495, 'n03026506': 496, 'n03028079': 497, 'n03032252': 498, 'n03041632': 499, 'n03042490': 500, 'n03045698': 501, 'n03047690': 502, 'n03062245': 503, 'n03063599': 504, 'n03063689': 505, 'n03065424': 506, 'n03075370': 507, 'n03085013': 508, 'n03089624': 509, 'n03095699': 510, 'n03100240': 511, 'n03109150': 512, 'n03110669': 513, 'n03124043': 514, 'n03124170': 515, 'n03125729': 516, 'n03126707': 517, 'n03127747': 518, 'n03127925': 519, 'n03131574': 520, 'n03133878': 521, 'n03134739': 522, 'n03141823': 523, 'n03146219': 524, 'n03160309': 525, 'n03179701': 526, 'n03180011': 527, 'n03187595': 528, 'n03188531': 529, 'n03196217': 530, 'n03197337': 531, 'n03201208': 532, 'n03207743': 533, 'n03207941': 534, 'n03208938': 535, 'n03216828': 536, 'n03218198': 537, 'n03220513': 538, 'n03223299': 539, 'n03240683': 540, 'n03249569': 541, 'n03250847': 542, 'n03255030': 543, 'n03259280': 544, 'n03271574': 545, 'n03272010': 546, 'n03272562': 547, 'n03290653': 548, 'n03291819': 549, 'n03297495': 550, 'n03314780': 551, 'n03325584': 552, 'n03337140': 553, 'n03344393': 554, 'n03345487': 555, 'n03347037': 556, 'n03355925': 557, 'n03372029': 558, 'n03376595': 559, 'n03379051': 560, 'n03384352': 561, 'n03388043': 562, 'n03388183': 563, 'n03388549': 564, 'n03393912': 565, 'n03394916': 566, 'n03400231': 567, 'n03404251': 568, 'n03417042': 569, 'n03424325': 570, 'n03425413': 571, 'n03443371': 572, 'n03444034': 573, 'n03445777': 574, 'n03445924': 575, 'n03447447': 576, 'n03447721': 577, 'n03450230': 578, 'n03452741': 579, 'n03457902': 580, 'n03459775': 581, 'n03461385': 582, 'n03467068': 583, 'n03476684': 584, 'n03476991': 585, 'n03478589': 586, 'n03481172': 587, 'n03482405': 588, 'n03483316': 589, 'n03485407': 590, 'n03485794': 591, 'n03492542': 592, 'n03494278': 593, 'n03495258': 594, 'n03496892': 595, 'n03498962': 596, 'n03527444': 597, 'n03529860': 598, 'n03530642': 599, 'n03532672': 600, 'n03534580': 601, 'n03535780': 602, 'n03538406': 603, 'n03544143': 604, 'n03584254': 605, 'n03584829': 606, 'n03590841': 607, 'n03594734': 608, 'n03594945': 609, 'n03595614': 610, 'n03598930': 611, 'n03599486': 612, 'n03602883': 613, 'n03617480': 614, 'n03623198': 615, 'n03627232': 616, 'n03630383': 617, 'n03633091': 618, 'n03637318': 619, 'n03642806': 620, 'n03649909': 621, 'n03657121': 622, 'n03658185': 623, 'n03661043': 624, 'n03662601': 625, 'n03666591': 626, 'n03670208': 627, 'n03673027': 628, 'n03676483': 629, 'n03680355': 630, 'n03690938': 631, 'n03691459': 632, 'n03692522': 633, 'n03697007': 634, 'n03706229': 635, 'n03709823': 636, 'n03710193': 637, 'n03710637': 638, 'n03710721': 639, 'n03717622': 640, 'n03720891': 641, 'n03721384': 642, 'n03724870': 643, 'n03729826': 644, 'n03733131': 645, 'n03733281': 646, 'n03733805': 647, 'n03742115': 648, 'n03743016': 649, 'n03759954': 650, 'n03761084': 651, 'n03763968': 652, 'n03764736': 653, 'n03769881': 654, 'n03770439': 655, 'n03770679': 656, 'n03773504': 657, 'n03775071': 658, 'n03775546': 659, 'n03776460': 660, 'n03777568': 661, 'n03777754': 662, 'n03781244': 663, 'n03782006': 664, 'n03785016': 665, 'n03786901': 666, 'n03787032': 667, 'n03788195': 668, 'n03788365': 669, 'n03791053': 670, 'n03792782': 671, 'n03792972': 672, 'n03793489': 673, 'n03794056': 674, 'n03796401': 675, 'n03803284': 676, 'n03804744': 677, 'n03814639': 678, 'n03814906': 679, 'n03825788': 680, 'n03832673': 681, 'n03837869': 682, 'n03838899': 683, 'n03840681': 684, 'n03841143': 685, 'n03843555': 686, 'n03854065': 687, 'n03857828': 688, 'n03866082': 689, 'n03868242': 690, 'n03868863': 691, 'n03871628': 692, 'n03873416': 693, 'n03874293': 694, 'n03874599': 695, 'n03876231': 696, 'n03877472': 697, 'n03877845': 698, 'n03884397': 699, 'n03887697': 700, 'n03888257': 701, 'n03888605': 702, 'n03891251': 703, 'n03891332': 704, 'n03895866': 705, 'n03899768': 706, 'n03902125': 707, 'n03903868': 708, 'n03908618': 709, 'n03908714': 710, 'n03916031': 711, 'n03920288': 712, 'n03924679': 713, 'n03929660': 714, 'n03929855': 715, 'n03930313': 716, 'n03930630': 717, 'n03933933': 718, 'n03935335': 719, 'n03937543': 720, 'n03938244': 721, 'n03942813': 722, 'n03944341': 723, 'n03947888': 724, 'n03950228': 725, 'n03954731': 726, 'n03956157': 727, 'n03958227': 728, 'n03961711': 729, 'n03967562': 730, 'n03970156': 731, 'n03976467': 732, 'n03976657': 733, 'n03977966': 734, 'n03980874': 735, 'n03982430': 736, 'n03983396': 737, 'n03991062': 738, 'n03992509': 739, 'n03995372': 740, 'n03998194': 741, 'n04004767': 742, 'n04005630': 743, 'n04008634': 744, 'n04009552': 745, 'n04019541': 746, 'n04023962': 747, 'n04026417': 748, 'n04033901': 749, 'n04033995': 750, 'n04037443': 751, 'n04039381': 752, 'n04040759': 753, 'n04041544': 754, 'n04044716': 755, 'n04049303': 756, 'n04065272': 757, 'n04067472': 758, 'n04069434': 759, 'n04070727': 760, 'n04074963': 761, 'n04081281': 762, 'n04086273': 763, 'n04090263': 764, 'n04099969': 765, 'n04111531': 766, 'n04116512': 767, 'n04118538': 768, 'n04118776': 769, 'n04120489': 770, 'n04125021': 771, 'n04127249': 772, 'n04131690': 773, 'n04133789': 774, 'n04136333': 775, 'n04141076': 776, 'n04141327': 777, 'n04141975': 778, 'n04146614': 779, 'n04147183': 780, 'n04149813': 781, 'n04152593': 782, 'n04153751': 783, 'n04154565': 784, 'n04162706': 785, 'n04179913': 786, 'n04192698': 787, 'n04200800': 788, 'n04201297': 789, 'n04204238': 790, 'n04204347': 791, 'n04208210': 792, 'n04209133': 793, 'n04209239': 794, 'n04228054': 795, 'n04229816': 796, 'n04235860': 797, 'n04238763': 798, 'n04239074': 799, 'n04243546': 800, 'n04251144': 801, 'n04252077': 802, 'n04252225': 803, 'n04254120': 804, 'n04254680': 805, 'n04254777': 806, 'n04258138': 807, 'n04259630': 808, 'n04263257': 809, 'n04264628': 810, 'n04265275': 811, 'n04266014': 812, 'n04270147': 813, 'n04273569': 814, 'n04275548': 815, 'n04277352': 816, 'n04285008': 817, 'n04286575': 818, 'n04296562': 819, 'n04310018': 820, 'n04311004': 821, 'n04311174': 822, 'n04317175': 823, 'n04325704': 824, 'n04326547': 825, 'n04328186': 826, 'n04330267': 827, 'n04332243': 828, 'n04335435': 829, 'n04336792': 830, 'n04344873': 831, 'n04346328': 832, 'n04347754': 833, 'n04350905': 834, 'n04355338': 835, 'n04355933': 836, 'n04356056': 837, 'n04357314': 838, 'n04366367': 839, 'n04367480': 840, 'n04370456': 841, 'n04371430': 842, 'n04371774': 843, 'n04372370': 844, 'n04376876': 845, 'n04380533': 846, 'n04389033': 847, 'n04392985': 848, 'n04398044': 849, 'n04399382': 850, 'n04404412': 851, 'n04409515': 852, 'n04417672': 853, 'n04418357': 854, 'n04423845': 855, 'n04428191': 856, 'n04429376': 857, 'n04435653': 858, 'n04442312': 859, 'n04443257': 860, 'n04447861': 861, 'n04456115': 862, 'n04458633': 863, 'n04461696': 864, 'n04462240': 865, 'n04465501': 866, 'n04467665': 867, 'n04476259': 868, 'n04479046': 869, 'n04482393': 870, 'n04483307': 871, 'n04485082': 872, 'n04486054': 873, 'n04487081': 874, 'n04487394': 875, 'n04493381': 876, 'n04501370': 877, 'n04505470': 878, 'n04507155': 879, 'n04509417': 880, 'n04515003': 881, 'n04517823': 882, 'n04522168': 883, 'n04523525': 884, 'n04525038': 885, 'n04525305': 886, 'n04532106': 887, 'n04532670': 888, 'n04536866': 889, 'n04540053': 890, 'n04542943': 891, 'n04548280': 892, 'n04548362': 893, 'n04550184': 894, 'n04552348': 895, 'n04553703': 896, 'n04554684': 897, 'n04557648': 898, 'n04560804': 899, 'n04562935': 900, 'n04579145': 901, 'n04579432': 902, 'n04584207': 903, 'n04589890': 904, 'n04590129': 905, 'n04591157': 906, 'n04591713': 907, 'n04592741': 908, 'n04596742': 909, 'n04597913': 910, 'n04599235': 911, 'n04604644': 912, 'n04606251': 913, 'n04612504': 914, 'n04613696': 915, 'n06359193': 916, 'n06596364': 917, 'n06785654': 918, 'n06794110': 919, 'n06874185': 920, 'n07248320': 921, 'n07565083': 922, 'n07579787': 923, 'n07583066': 924, 'n07584110': 925, 'n07590611': 926, 'n07613480': 927, 'n07614500': 928, 'n07615774': 929, 'n07684084': 930, 'n07693725': 931, 'n07695742': 932, 'n07697313': 933, 'n07697537': 934, 'n07711569': 935, 'n07714571': 936, 'n07714990': 937, 'n07715103': 938, 'n07716358': 939, 'n07716906': 940, 'n07717410': 941, 'n07717556': 942, 'n07718472': 943, 'n07718747': 944, 'n07720875': 945, 'n07730033': 946, 'n07734744': 947, 'n07742313': 948, 'n07745940': 949, 'n07747607': 950, 'n07749582': 951, 'n07753113': 952, 'n07753275': 953, 'n07753592': 954, 'n07754684': 955, 'n07760859': 956, 'n07768694': 957, 'n07802026': 958, 'n07831146': 959, 'n07836838': 960, 'n07860988': 961, 'n07871810': 962, 'n07873807': 963, 'n07875152': 964, 'n07880968': 965, 'n07892512': 966, 'n07920052': 967, 'n07930864': 968, 'n07932039': 969, 'n09193705': 970, 'n09229709': 971, 'n09246464': 972, 'n09256479': 973, 'n09288635': 974, 'n09332890': 975, 'n09399592': 976, 'n09421951': 977, 'n09428293': 978, 'n09468604': 979, 'n09472597': 980, 'n09835506': 981, 'n10148035': 982, 'n10565667': 983, 'n11879895': 984, 'n11939491': 985, 'n12057211': 986, 'n12144580': 987, 'n12267677': 988, 'n12620546': 989, 'n12768682': 990, 'n12985857': 991, 'n12998815': 992, 'n13037406': 993, 'n13040303': 994, 'n13044778': 995, 'n13052670': 996, 'n13054560': 997, 'n13133613': 998, 'n15075141': 999}\n"
     ]
    }
   ],
   "source": [
    "img_height = IMAGE_SIZE\n",
    "img_width = IMAGE_SIZE\n",
    "batch_size = BATCH_SIZE\n",
    "AUTOTUNE = tf.data.AUTOTUNE # ให้ TensorFlow จัดการ parallelism ที่เหมาะสมเอง\n",
    "\n",
    "# # --- ส่วนของการลบ bad_dirs---\n",
    "# bad_dirs = ['.ipynb_checkpoints']\n",
    "# for bad in bad_dirs:\n",
    "#     path_to_check = os.path.join(datapath, bad) # แก้ไขเป็น path_to_check เพื่อไม่ให้ทับซ้อนกับตัวแปร path อื่น\n",
    "#     if os.path.exists(path_to_check):\n",
    "#         print(f\"Removing directory: {path_to_check}\")\n",
    "#         shutil.rmtree(path_to_check)\n",
    "\n",
    "# --- 1. โหลดชุดข้อมูลโดยใช้ image_dataset_from_directory ---\n",
    "# ฟังก์ชันนี้จะจัดการการอ่านจากไดเรกทอรี, กำหนดขนาดภาพ, สร้าง batch, และแบ่งข้อมูล\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    datapath,\n",
    "    validation_split=0.04,  # ตรงกับ validation_split เดิมของคุณ\n",
    "    subset=\"training\",\n",
    "    seed=42,               # ตรงกับ seed เดิม\n",
    "    image_size=(img_height, img_width), # ตรงกับ target_size เดิม\n",
    "    batch_size=batch_size, # โหลดเป็น batch เลย\n",
    "    label_mode='categorical', # ตรงกับ class_mode เดิม (labels เป็น one-hot)\n",
    "    shuffle=True # สุ่มข้อมูลใน training set\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    datapath,\n",
    "    validation_split=0.04,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=False # ไม่ต้องสุ่มใน validation set\n",
    ")\n",
    "\n",
    "# แสดง class names และสร้าง class_indices (คล้ายกับ .class_indices ของ ImageDataGenerator)\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "print(\"Class indices:\", class_indices)\n",
    "\n",
    "\n",
    "# --- 2. สร้าง Model สำหรับ Data Augmentation โดยใช้ Keras Preprocessing Layers ---\n",
    "data_augmentation_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255), # ทำ Rescaling เป็นอันดับแรก\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(factor=(-10/360, 10/360), fill_mode='nearest'),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode='nearest'),\n",
    "    tf.keras.layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1), fill_mode='nearest')    \n",
    "])\n",
    "\n",
    "# สร้าง Layer สำหรับ Rescale อย่างเดียว (สำหรับ validation set)\n",
    "rescale_layer_only = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# --- 3. สร้างฟังก์ชันสำหรับนำ Augmentation และ Preprocessing ไปใช้ ---\n",
    "\n",
    "def augment_and_preprocess_train_data(images, labels):\n",
    "    images = data_augmentation_layers(images, training=True) # training=True เพื่อให้ Random* layers ทำงาน\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_val_data(images, labels):\n",
    "    images = rescale_layer_only(images) # Validation data ทำแค่ rescale\n",
    "    return images, labels\n",
    "\n",
    "# --- 4. สร้าง Input Pipelines ที่มีประสิทธิภาพ ---\n",
    "# Training pipeline\n",
    "train_pipeline = train_dataset.map(augment_and_preprocess_train_data, num_parallel_calls=AUTOTUNE)\n",
    "train_pipeline = train_pipeline.prefetch(buffer_size=AUTOTUNE) # ให้ CPU เตรียมข้อมูลล่วงหน้า\n",
    "\n",
    "# Validation pipeline\n",
    "val_pipeline = val_dataset.map(preprocess_val_data, num_parallel_calls=AUTOTUNE)\n",
    "val_pipeline = val_pipeline.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# # --- ตรวจสอบ Output ของ Datasets ---\n",
    "# print(\"\\n--- ตรวจสอบ Output ของ Datasets (หลัง Preprocessing) ---\")\n",
    "# for X_batch_train, y_batch_train in train_pipeline.take(1):\n",
    "#     print(\"Shape of first BATCH of TRAIN images:\", X_batch_train.shape)\n",
    "#     print(\"Data type of TRAIN images:\", X_batch_train.dtype)\n",
    "#     print(\"Min value in TRAIN images:\", tf.reduce_min(X_batch_train).numpy())\n",
    "#     print(\"Max value in TRAIN images:\", tf.reduce_max(X_batch_train).numpy())\n",
    "#     print(\"Shape of first BATCH of TRAIN labels:\", y_batch_train.shape)\n",
    "\n",
    "# for X_batch_val, y_batch_val in val_pipeline.take(1):\n",
    "#     print(\"Shape of first BATCH of VAL images:\", X_batch_val.shape)\n",
    "#     print(\"Data type of VAL images:\", X_batch_val.dtype)\n",
    "#     print(\"Min value in VAL images:\", tf.reduce_min(X_batch_val).numpy())\n",
    "#     print(\"Max value in VAL images:\", tf.reduce_max(X_batch_val).numpy())\n",
    "#     print(\"Shape of first BATCH of VAL labels:\", y_batch_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3cf557",
   "metadata": {},
   "source": [
    "# Caculate steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb1ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples (tf.data): 1229921\n",
      "Batch size (tf.data): 32\n",
      "Steps per epoch for training (tf.data): 38436\n",
      "Total validation samples (tf.data): 51246\n",
      "Validation steps (tf.data): 1602\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = 1229921 \n",
    "num_val_samples = 51246    \n",
    "# batch_size ควรเป็นตัวแปรที่คุณกำหนดค่าไว้ เช่น batch_size = 32\n",
    "\n",
    "# คำนวณ steps_per_epoch (ต้องปัดขึ้น)\n",
    "steps_per_epoch = math.ceil(num_train_samples / batch_size)\n",
    "print(f\"Total training samples (tf.data): {num_train_samples}\")\n",
    "print(f\"Batch size (tf.data): {batch_size}\")\n",
    "print(f\"Steps per epoch for training (tf.data): {steps_per_epoch}\")\n",
    "\n",
    "# คำนวณ validation_steps (ต้องปัดขึ้น)\n",
    "if num_val_samples > 0:\n",
    "    validation_steps= math.ceil(num_val_samples / batch_size)\n",
    "    print(f\"Total validation samples (tf.data): {num_val_samples}\")\n",
    "    print(f\"Validation steps (tf.data): {validation_steps}\")\n",
    "else:\n",
    "    validation_steps_new = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fc191",
   "metadata": {},
   "source": [
    "## ENCODEBLOCK & VISIONTRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b96935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected Transformer Encoder Block with Pre-LN (Layer Normalization before sub-layer)\n",
    "class EncoderBlock(Model):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Layer Normalization 1 (before Multi-Head Attention)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6, name=\"norm1\")\n",
    "        # Multi-Head Attention layer\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim // num_heads, # key_dim per head\n",
    "            dropout=dropout_rate, # Dropout within MHA's attention scores\n",
    "            name=\"multi_head_attention\"\n",
    "        )\n",
    "       \n",
    "        self.dropout_mha_output = Dropout(dropout_rate)\n",
    "\n",
    "        # Layer Normalization 2 (before MLP)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6, name=\"norm2\")\n",
    "        # MLP Block (Feed-Forward Network)\n",
    "        self.mlp = Sequential([\n",
    "            Dense(mlp_dim, activation='gelu', name=\"mlp_dense_1\"),\n",
    "            Dropout(dropout_rate), # Dropout within MLP\n",
    "            Dense(embed_dim, name=\"mlp_dense_2\"),\n",
    "            Dropout(dropout_rate)  # Dropout after projection, before adding to residual\n",
    "        ], name=\"mlp_block\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # --- Multi-Head Attention sub-layer (Pre-LN) ---\n",
    "        # 1. Layer Normalization\n",
    "        x_norm1 = self.norm1(inputs)\n",
    "        # 2. Multi-Head Attention (produces attention_output)\n",
    "        attn_output = self.mha(query=x_norm1, value=x_norm1, key=x_norm1, training=training)\n",
    "        # 3. Dropout on MHA output (optional, but often used)\n",
    "        attn_output_dropped = self.dropout_mha_output(attn_output, training=training)\n",
    "        # 4. Residual connection\n",
    "        x_res1 = inputs + attn_output_dropped\n",
    "\n",
    "        # --- MLP sub-layer (Pre-LN) ---\n",
    "        # 1. Layer Normalization\n",
    "        x_norm2 = self.norm2(x_res1) # Apply norm to the output of the first residual connection\n",
    "        # 2. MLP (produces mlp_output)\n",
    "        mlp_output = self.mlp(x_norm2, training=training)\n",
    "        # 3. Residual connection (MLP already has internal dropout before output)\n",
    "        x_res2 = x_res1 + mlp_output\n",
    "\n",
    "        return x_res2\n",
    "\n",
    "# Vision Transformer (ViT) model implementation (Unchanged from your provided code, but uses the modified EncoderBlock)\n",
    "class VisionTransformer(Model):\n",
    "    def __init__(self, image_size=IMAGE_SIZE, num_classes=NUM_CLASSES, patch_size=PATCH_SIZE, embed_dim=EMBED_DIM,\n",
    "                 num_heads=NUM_HEADS, num_layers=NUM_LAYERS, mlp_dim=MLP_DIM, dropout_rate=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # 1. CLS Token\n",
    "        self.cls_token = self.add_weight(\n",
    "            name = \"cls_token\",\n",
    "            shape=[1, 1, embed_dim], # (1, 1, D)\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # 2. Position Embedding\n",
    "        self.pos_embed = self.add_weight(\n",
    "            name = \"position_embedding\",\n",
    "            shape=[1, self.num_patches + 1, embed_dim], # (1, num_patches + 1, D)\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.pos_dropout = Dropout(dropout_rate)\n",
    "\n",
    "        # Patch embedding layer\n",
    "        self.patch_embed = Conv2D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding='valid',\n",
    "            name=\"patch_embed\"\n",
    "        )\n",
    "\n",
    "        # Transformer Encoder Layers (Now uses the Pre-LN EncoderBlock)\n",
    "        self.encoder_layers = [\n",
    "            EncoderBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Classifier head\n",
    "        self.norm_head = LayerNormalization(epsilon=1e-6, name=\"head_norm\") # Norm ก่อนเข้า MLP Head\n",
    "        self.head = Dense(num_classes, activation='softmax', name=\"classification_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # 1. Embed patches\n",
    "        x = self.patch_embed(inputs)  # (B, H/P, W/P, D)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.embed_dim))  # (B, num_patches, D)\n",
    "\n",
    "        # 2. Prepend CLS token\n",
    "        cls_tokens = tf.tile(self.cls_token, [batch_size, 1, 1])  # (B, 1, D)\n",
    "        x = tf.concat([cls_tokens, x], axis=1)  # (B, num_patches + 1, D)\n",
    "\n",
    "        # 3. Add position embedding\n",
    "        x = x + self.pos_embed  # Broadcasting\n",
    "        x = self.pos_dropout(x, training=training)\n",
    "\n",
    "        # 4. Transformer Encoder layers\n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x, training=training)\n",
    "\n",
    "        # 5. Classifier head\n",
    "        # Take the output of the CLS token (first token)\n",
    "        cls_token_output = x[:, 0]\n",
    "        cls_token_output = self.norm_head(cls_token_output, training=training) # Norm output of CLS token\n",
    "        logits = self.head(cls_token_output, training=training) # Keras Dense layer handles training flag for activations if needed\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f55bb3",
   "metadata": {},
   "source": [
    "# WarmupCosineDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, decay_steps, warmup_steps, alpha=0.0, name=None):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.initial_learning_rate = initial_learning_rate # LR สูงสุดหลัง warmup\n",
    "        self.decay_steps = decay_steps                     # จำนวน steps ทั้งหมดสำหรับการ decay\n",
    "        self.warmup_steps = warmup_steps                   # จำนวน steps สำหรับ warmup\n",
    "        self.alpha = alpha                                 # LR ต่ำสุด = initial_learning_rate * alpha\n",
    "\n",
    "        # Cosine decay part starts after warmup\n",
    "        self.cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate=self.initial_learning_rate,\n",
    "            decay_steps=self.decay_steps - self.warmup_steps, # จำนวน steps สำหรับ cosine decay part\n",
    "            alpha=self.alpha\n",
    "        )\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step_float = tf.cast(step, tf.float32)\n",
    "        warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "\n",
    "        # Linear warmup phase\n",
    "        def warmup_fn():\n",
    "            return (self.initial_learning_rate / warmup_steps_float) * step_float\n",
    "\n",
    "        # Cosine decay phase (adjust step for cosine_decay_schedule)\n",
    "        def cosine_decay_fn():\n",
    "            return self.cosine_decay_schedule(step_float - warmup_steps_float)\n",
    "\n",
    "        learning_rate = tf.cond(\n",
    "            step_float < warmup_steps_float,\n",
    "            warmup_fn,\n",
    "            cosine_decay_fn\n",
    "        )\n",
    "        return learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4971b",
   "metadata": {},
   "source": [
    "# Model setup and Vision Transformer compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51218921",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = EPOCHS * steps_per_epoch\n",
    "warmup_epochs = 10  # จำนวน epochs สำหรับ warmup\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "peak_learning_rate = 1e-4 # หรือ 1e-3, 3e-4 (LR สูงสุดที่จะใช้)\n",
    "weight_decay_rate = 0.05  # ค่า weight decay สำหรับ AdamW (อาจต้องปรับจูน)\n",
    "\n",
    "# สร้าง instance ของ Learning Rate Schedule\n",
    "lr_schedule = WarmupCosineDecay(\n",
    "    initial_learning_rate=peak_learning_rate,\n",
    "    decay_steps=total_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    alpha=0.0 # ลด LR ลงจนเกือบเป็น 0 เมื่อสิ้นสุด\n",
    ")\n",
    "\n",
    "try:\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        weight_decay=weight_decay_rate,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "except AttributeError:\n",
    "    print(\"tf.keras.optimizers.AdamW not found, trying experimental version.\")\n",
    "    try:\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=weight_decay_rate\n",
    "        )\n",
    "    except AttributeError:\n",
    "        print(\"Experimental AdamW not found. Please ensure TensorFlow version is compatible or install tensorflow-addons.\")\n",
    "        \n",
    "# Build the model\n",
    "model = VisionTransformer(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    mlp_dim=MLP_DIM,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(f\"Optimizer set to AdamW with WarmupCosineDecay schedule.\")\n",
    "print(f\"Total training steps: {total_steps}, Warmup steps: {warmup_steps}, Peak LR: {peak_learning_rate}\")\n",
    "\n",
    "dummy_input = tf.zeros((1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "# เรียก Model ด้วย Dummy Input นี้ จะเป็นการบังคับให้เมธอด call() ทำงาน และ build เลเยอร์ภายในทั้งหมด\n",
    "_ = model(dummy_input) # ผลลัพธ์จากการเรียกนี้เราไม่จำเป็นต้องใช้ จึงกำหนดให้ _\n",
    "# สร้าง EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5, # จำนวน epoch ที่จะรอถ้า val_loss ไม่ดีขึ้น\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_path = 'best_vit_model_1000_classes.keras'\n",
    "# สร้าง ModelCheckpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=model_checkpoint_path, # หรือ .h5 หรือ tf format\n",
    "    monitor='val_loss',      # เกณฑ์ที่ใช้ในการเลือกโมเดลที่ดีที่สุด\n",
    "    save_best_only=True,     # บันทึกเฉพาะโมเดลที่ดีที่สุด\n",
    "    save_weights_only=False, # บันทึกทั้งสถาปัตยกรรมและน้ำหนัก (ถ้า False) หรือเฉพาะน้ำหนัก (ถ้า True)\n",
    "    verbose=1                # แสดงข้อความเมื่อมีการบันทึก\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "\n",
    "model.summary() # ดูสรุปโมเดล"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae72d20",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d01bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(f\"Starting training with image_size={IMAGE_SIZE}, patch_size={PATCH_SIZE}, embed_dim={EMBED_DIM}\")\n",
    "history = model.fit(train_pipeline,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=val_pipeline,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps ,\n",
    "                    callbacks=[early_stopping, model_checkpoint,csv_logger],\n",
    "                    )\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da358738",
   "metadata": {},
   "source": [
    "# Evaluate the model (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating the BEST model saved by ModelCheckpoint ---\")\n",
    "# โหลดโมเดลที่ดีที่สุดที่บันทึกโดย ModelCheckpoint\n",
    "# best_model_path ควรตรงกับ filepath ใน ModelCheckpoint callback\n",
    "best_model_path = model_checkpoint_path # หรือ .h5 หรือ tf format\n",
    "if os.path.exists(best_model_path):\n",
    "    best_model = load_model(best_model_path)\n",
    "\n",
    "    # ไม่จำเป็นต้อง compile ใหม่ถ้า .keras file บันทึกสถานะ optimizer ไว้แล้ว\n",
    "    # แต่ถ้าต้องการความแน่นอน หรือมีการเปลี่ยน custom objects/metrics ก็สามารถ compile ใหม่ได้\n",
    "    best_model.compile(optimizer=optimizer # ใช้ LR ที่เหมาะสม\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "    val_loss, val_accuracy = best_model.evaluate(val_pipeline)\n",
    "    print(f\"Validation Loss (Best Model): {val_loss}\")\n",
    "    print(f\"Validation Accuracy (Best Model): {val_accuracy}\")\n",
    "else:\n",
    "    print(f\"Error: Best model file '{best_model_path}' not found. Training might not have completed or saved a model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
