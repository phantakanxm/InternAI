{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162c5575",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.callbacks import CSVLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77599e2",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! กรุณาแก้ไข datapath ให้เป็น path ที่ถูกต้องไปยังชุดข้อมูลของคุณ !!!\n",
    "train_path = '/media/capybara/Data/dataset_vit/archive'\n",
    "datapath = train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c8080",
   "metadata": {},
   "source": [
    "# Preprocessing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a70f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- กำหนดค่าพารามิเตอร์เริ่มต้น ---\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# --- 1. โหลดชุดข้อมูลโดยใช้ image_dataset_from_directory ---\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    datapath,\n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True, \n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    datapath,\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "num_classes = len(class_names)\n",
    "print(\"\\n--- ตรวจสอบข้อมูลชุดข้อมูล ---\")\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# --- 2. สร้าง Model สำหรับ Data Augmentation และ ResNet50 Preprocessing ---\n",
    "# Import ฟังก์ชัน preprocess_input ของ ResNet50\n",
    "data_augmentation_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(factor=(-10/360, 10/360), fill_mode='nearest'),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode='nearest'),\n",
    "    tf.keras.layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1), fill_mode='nearest')\n",
    "])\n",
    "\n",
    "# --- 3. สร้างฟังก์ชันสำหรับนำ Augmentation และ Preprocessing ไปใช้ ---\n",
    "def augment_and_preprocess_train_data(images, labels):\n",
    "    images = data_augmentation_layers(images, training=True)\n",
    "    images = resnet_preprocess_input(images)\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_val_data(images, labels):\n",
    "    images = resnet_preprocess_input(images)\n",
    "    return images, labels\n",
    "\n",
    "# --- 4. สร้าง Input Pipelines ที่มีประสิทธิภาพ ---\n",
    "train_pipeline = train_dataset.map(augment_and_preprocess_train_data, num_parallel_calls=AUTOTUNE)\n",
    "train_pipeline = train_pipeline.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_pipeline = val_dataset.map(preprocess_val_data, num_parallel_calls=AUTOTUNE)\n",
    "val_pipeline = val_pipeline.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# # --- ตรวจสอบ Output ของ Datasets ---\n",
    "# print(\"\\n--- ตรวจสอบ Output ของ Datasets (หลัง Preprocessing) ---\")\n",
    "# for X_batch_train, y_batch_train in train_pipeline.take(1):\n",
    "#     print(\"Shape of first BATCH of TRAIN images:\", X_batch_train.shape)\n",
    "#     print(\"Data type of TRAIN images:\", X_batch_train.dtype)\n",
    "#     print(\"Min value in TRAIN images:\", tf.reduce_min(X_batch_train).numpy())\n",
    "#     print(\"Max value in TRAIN images:\", tf.reduce_max(X_batch_train).numpy())\n",
    "#     print(\"Shape of first BATCH of TRAIN labels:\", y_batch_train.shape)\n",
    "\n",
    "# for X_batch_val, y_batch_val in val_pipeline.take(1):\n",
    "#     print(\"Shape of first BATCH of VAL images:\", X_batch_val.shape)\n",
    "#     print(\"Data type of VAL images:\", X_batch_val.dtype)\n",
    "#     print(\"Min value in VAL images:\", tf.reduce_min(X_batch_val).numpy())\n",
    "#     print(\"Max value in VAL images:\", tf.reduce_max(X_batch_val).numpy())\n",
    "#     print(\"Shape of first BATCH of VAL labels:\", y_batch_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab263904",
   "metadata": {},
   "source": [
    "# Load the Pre-trained ResNet50 (Feature Extractor) and retrain only the Head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07fe92",
   "metadata": {},
   "source": [
    "## Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121bceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- กำหนดค่าพื้นฐาน ---\n",
    "IMAGE_SIZE = (224, 224)\n",
    "INITIAL_LR = 1e-3\n",
    "FINE_TUNE_LR = 1e-5\n",
    "\n",
    "# --- ขั้นตอนที่ 1: โหลด Pre-trained ResNet50 (Feature Extractor) และฝึกสอนเฉพาะ Head ใหม่ ---\n",
    "\n",
    "# === โหลด ResNet50 Pre-trained บน ImageNet, ไม่รวม Head เดิม ===\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=IMAGE_SIZE + (3,))\n",
    "\n",
    "# === Freeze Backbone Layers ===\n",
    "base_model.trainable = False # Freeze layers ของ ResNet50 เดิมทั้งหมด\n",
    "\n",
    "# === สร้าง Head ใหม่ ===\n",
    "# ต่อ GlobalAveragePooling2D เพื่อลดมิติ ต่อด้วย Dense layer สำหรับ classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "x = Dense(1024, activation='relu', name='fc1')(x) # อาจจะมี Dense layer ขั้นกลาง\n",
    "x = Dropout(0.5, name='dropout')(x) # เพิ่ม Dropout เพื่อป้องกัน Overfitting\n",
    "predictions = Dense(num_classes, activation='softmax', name='custom_classifier')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.summary() # ดูโครงสร้างโมเดล\n",
    "\n",
    "# === Compile โมเดลสำหรับ Phase 1 ===\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=INITIAL_LR),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5, # จำนวน epoch ที่จะรอถ้า val_loss ไม่ดีขึ้น\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# สร้าง ModelCheckpoint callback\n",
    "model_checkpoint_path = 'best_resnet_model_imagenet2012v2_classes.keras' # หรือ .h5 หรือ tf format\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=model_checkpoint_path, \n",
    "    monitor='val_loss',      # เกณฑ์ที่ใช้ในการเลือกโมเดลที่ดีที่สุด\n",
    "    save_best_only=True,     # บันทึกเฉพาะโมเดลที่ดีที่สุด\n",
    "    save_weights_only=False, # บันทึกทั้งสถาปัตยกรรมและน้ำหนัก (ถ้า False) หรือเฉพาะน้ำหนัก (ถ้า True)\n",
    "    verbose=1                # แสดงข้อความเมื่อมีการบันทึก\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "\n",
    "\n",
    "callback_list_phase1 = [early_stopping, model_checkpoint, csv_logger]\n",
    "print(\"--- เริ่ม Phase 1: Training the new head (ResNet50) ---\")\n",
    "# --- Training Phase 1 ---\n",
    "num_epochs_phase1 = 10 # กำหนดจำนวน epochs\n",
    "history_phase1 = model.fit(\n",
    "    train_pipeline, \n",
    "    epochs=num_epochs_phase1,\n",
    "    validation_data=val_pipeline, \n",
    "    callbacks=callback_list_phase1\n",
    ")\n",
    "print(\"--- จบ Phase 1 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b6235",
   "metadata": {},
   "source": [
    "## Phase 2 Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ขั้นตอนที่ 2: Unfreeze Backbone (หรือบางส่วน) และ Fine-tune ทั้งโมเดล ---\n",
    "print(\"\\n Loading the best model from Phase 1 for fine-tuning...\")\n",
    "model.load_weights(model_checkpoint_path) # โหลดโมเดลที่ดีที่สุดจาก Phase 1\n",
    "# === Unfreeze Backbone Layers ===\n",
    "base_model.trainable = True\n",
    "\n",
    "fine_tune_at = 143 # Index ของ layer 'conv5_block1_0_conv' ใน ResNet50 (อาจจะต้องเช็คชื่อ layer อีกครั้ง)\n",
    "print(f\"Fine-tuning from layer index {fine_tune_at} ('{base_model.layers[fine_tune_at].name}') onwards.\")\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "   layer.trainable = False\n",
    "\n",
    "# === Re-compile โมเดลสำหรับ Phase 2 ด้วย learning rate ที่ต่ำกว่า ===\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=FINE_TUNE_LR), # Learning rate ต่ำมาก\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary() # ดูว่าพารามิเตอร์ที่ trainable เพิ่มขึ้น\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2, # ลด learning rate ลง 10 เท่า\n",
    "    patience=2, # จำนวน epoch ที่จะรอถ้า val_loss ไม่ดีขึ้น\n",
    "    min_lr=1e-7, # ค่าต่ำสุดของ learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# log_dir_phase2 = \"logs/fit/phase2_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback_phase2 = TensorBoard(log_dir=log_dir_phase2, histogram_freq=1)\n",
    "\n",
    "callback_list_phase2 = [early_stopping, model_checkpoint, reduce_lr, csv_logger]\n",
    "\n",
    "print(\"\\n--- เริ่ม Phase 2: Fine-tuning the entire model (ResNet50) ---\")\n",
    "# --- Training Phase 2 ---\n",
    "num_epochs_phase2 = 20 # กำหนดจำนวน epochs\n",
    "# # ถ้า train ต่อจาก phase 1:\n",
    "if history_phase1.epoch:\n",
    "    initial_epoch_phase2 = history_phase1.epoch[-1] + 1 # เริ่มจาก epoch ถัดไป\n",
    "else:\n",
    "    initial_epoch_phase2 = 0\n",
    "\n",
    "total_epochs_overall = initial_epoch_phase2 + num_epochs_phase2\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_pipeline,\n",
    "    epochs=total_epochs_overall, \n",
    "    initial_epoch=initial_epoch_phase2, # ถ้า train ต่อ\n",
    "    validation_data=val_pipeline,\n",
    "    callbacks=callback_list_phase2\n",
    ")\n",
    "print(\"--- จบ Phase 2 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca55ef9",
   "metadata": {},
   "source": [
    "# Evaluate the model for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b583db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating the BEST model saved by ModelCheckpoint ---\")\n",
    "# โหลดโมเดลที่ดีที่สุดที่บันทึกโดย ModelCheckpoint\n",
    "# best_model_path ควรตรงกับ filepath ใน ModelCheckpoint callback\n",
    "best_model_path = 'best_resnet_model_imagenet2012v2_classes.keras' # หรือ .h5 หรือ tf format\n",
    "if os.path.exists(best_model_path):\n",
    "    best_model = load_model(best_model_path)\n",
    "\n",
    "    # ไม่จำเป็นต้อง compile ใหม่ถ้า .keras file บันทึกสถานะ optimizer ไว้แล้ว\n",
    "    # แต่ถ้าต้องการความแน่นอน หรือมีการเปลี่ยน custom objects/metrics ก็สามารถ compile ใหม่ได้\n",
    "    best_model.compile(optimizer=Adam(learning_rate=FINE_TUNE_LR), # ใช้ LR ที่เหมาะสม\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "    val_loss, val_accuracy = best_model.evaluate(val_pipeline)\n",
    "    print(f\"Validation Loss (Best Model): {val_loss}\")\n",
    "    print(f\"Validation Accuracy (Best Model): {val_accuracy}\")\n",
    "else:\n",
    "    print(f\"Error: Best model file '{best_model_path}' not found. Training might not have completed or saved a model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
